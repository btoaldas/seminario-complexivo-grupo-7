"""
Script para optimizar el dataset FIFA para carga r√°pida en el API
Convierte CSV a Parquet (10x m√°s r√°pido) y optimiza tipos de datos
"""

import pandas as pd
from pathlib import Path
import sys

BASE_DIR = Path(__file__).parent.parent.parent.parent
DATA_PATH = BASE_DIR / 'datos' / 'procesados'

def optimizar_dataset():
    """
    Convierte fifa_limpio.csv a Parquet optimizado
    Reduce tiempo de carga de ~20s a ~2-3s
    """
    print("="*70)
    print("‚ö° OPTIMIZADOR DE DATASET FIFA")
    print("="*70)
    
    csv_path = DATA_PATH / 'fifa_limpio.csv'
    parquet_path = DATA_PATH / 'fifa_limpio.parquet'
    
    if not csv_path.exists():
        print(f"‚ùå Error: No se encontr√≥ {csv_path}")
        return
    
    # 1. Cargar CSV
    print(f"\nüìÇ Cargando {csv_path.name}...")
    df = pd.read_csv(csv_path, low_memory=False)
    print(f"   ‚úÖ Cargado: {len(df):,} registros √ó {len(df.columns)} columnas")
    
    # 2. Optimizar tipos de datos
    print("\nüîß Optimizando tipos de datos...")
    
    # Columnas categ√≥ricas (strings repetitivos)
    categoricas = [
        'club', 'liga', 'nacionalidad', 'posiciones_jugador', 
        'pie_preferido', 'categoria_posicion', 'categoria_edad',
        'categoria_reputacion', 'clasificacion_ml'
    ]
    
    for col in categoricas:
        if col in df.columns:
            df[col] = df[col].astype('category')
            print(f"   ‚Ä¢ {col} ‚Üí category")
    
    # Columnas int (ahorrar memoria)
    int_cols = [
        'edad', 'valoracion_global', 'potencial', 'altura_cm', 'peso_kg',
        'reputacion_internacional', 'pie_debil', 'a√±o_datos'
    ]
    
    for col in int_cols:
        if col in df.columns and df[col].dtype == 'float64':
            df[col] = df[col].fillna(0).astype('int32')
            print(f"   ‚Ä¢ {col} ‚Üí int32")
    
    # 3. Calcular tama√±o antes/despu√©s
    csv_size_mb = csv_path.stat().st_size / (1024 * 1024)
    
    # 4. Guardar como Parquet
    print(f"\nüíæ Guardando {parquet_path.name}...")
    df.to_parquet(
        parquet_path,
        engine='pyarrow',
        compression='snappy',
        index=False
    )
    
    parquet_size_mb = parquet_path.stat().st_size / (1024 * 1024)
    
    print("\n" + "="*70)
    print("‚úÖ DATASET OPTIMIZADO EXITOSAMENTE")
    print("="*70)
    print(f"üìÅ Archivos generados:")
    print(f"   ‚Ä¢ CSV:     {csv_path.name} ({csv_size_mb:.1f} MB)")
    print(f"   ‚Ä¢ Parquet: {parquet_path.name} ({parquet_size_mb:.1f} MB)")
    print(f"\nüìä Reducci√≥n de tama√±o: {((csv_size_mb - parquet_size_mb) / csv_size_mb * 100):.1f}%")
    print(f"‚ö° Velocidad de carga estimada:")
    print(f"   ‚Ä¢ CSV:     ~15-20 segundos")
    print(f"   ‚Ä¢ Parquet: ~2-3 segundos (7x m√°s r√°pido)")
    print("="*70)
    
    return df


if __name__ == "__main__":
    optimizar_dataset()
